import { nanoid } from "nanoid";

/**
 * Document test fixtures for RAG integration tests
 */

export interface DocumentFixture {
  filename: string;
  content: string;
  mimeType: string;
  size: number;
}

/**
 * Sample documents for testing ingestion pipeline
 */
export const SAMPLE_DOCUMENTS = {
  /**
   * Short article about TypeScript
   * ~500 tokens, good for testing chunking
   */
  typescript_intro: {
    filename: "typescript-intro.txt",
    mimeType: "text/plain",
    content: `TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.

What is TypeScript?
TypeScript is JavaScript with syntax for types. TypeScript is a strongly typed programming language that builds on JavaScript, giving you better tooling at any scale. TypeScript adds optional types to JavaScript that support tools for large-scale JavaScript applications for any browser, for any host, and for any OS.

Why TypeScript?
1. Types improve code quality and understandability
2. With types, you can make far fewer errors
3. The TypeScript compiler checks types and reports errors
4. Your IDE can use this information to provide code hints and intelligent code completion
5. Types serve as great documentation
6. You can catch bugs during development instead of at runtime

Getting Started
To use TypeScript, you need to install it first. You can install TypeScript using npm:
npm install -g typescript

Once installed, you can create a .ts file and write TypeScript code. Then compile it using:
tsc filename.ts

This will generate a corresponding .js file that can be run by any JavaScript runtime.

Key Features
1. Optional Static Typing - Types are optional, allowing for gradual adoption
2. Type Inference - TypeScript can infer types from your code
3. Interfaces - Define contracts for objects and classes
4. Classes and Inheritance - Full OOP support
5. Enums - Define a set of named constants
6. Generics - Create reusable components
7. Decorators - Add metadata to classes and properties
8. Namespaces - Organize code into logical groups

Conclusion
TypeScript has become the language of choice for large-scale JavaScript development. Major frameworks like Angular, React with TypeScript support, and Vue all have strong TypeScript integration. If you're building a modern web application, TypeScript is worth learning.`,
  } as DocumentFixture,

  /**
   * Technical documentation about vector databases
   * ~600 tokens, tests longer content
   */
  vector_db_guide: {
    filename: "vector-database-guide.txt",
    mimeType: "text/plain",
    content: `Vector Databases: A Comprehensive Guide

Introduction
Vector databases are a new class of databases designed to handle high-dimensional vector data and perform similarity searches efficiently. They power modern AI applications including semantic search, recommendation systems, and retrieval-augmented generation (RAG).

What is a Vector?
In the context of databases, a vector is a numerical representation of data. For example:
- Text embeddings: 384-dimensional vectors representing sentences or documents
- Image embeddings: High-dimensional vectors capturing visual features
- Audio embeddings: Vectors representing audio characteristics

These vectors are typically generated by machine learning models like transformers or CLIP.

Why Vector Databases?
Traditional SQL databases are not designed for similarity searches. Vector databases provide:
1. Efficient similarity search using various distance metrics
2. Scalability to millions or billions of vectors
3. Real-time querying for interactive applications
4. Support for filtering alongside vector search

Common Vector Databases
1. Milvus - Open-source, distributed vector database
2. Pinecone - Fully managed cloud vector database
3. Weaviate - AI-native vector database with GraphQL API
4. Qdrant - High-performance vector database with strict filtering
5. Vespa - Enterprise-grade vector database

Distance Metrics
Vector similarity is measured using distance metrics:
- Euclidean Distance (L2): Measures straight-line distance
- Cosine Similarity: Measures angle between vectors, ideal for normalized embeddings
- Inner Product (IP): Fast computation, good for pre-normalized vectors
- Hamming Distance: For binary vectors

Indexing Strategies
1. Flat Index: Exact search, no compression, good for small datasets
2. HNSW: Hierarchical Navigable Small World, excellent for most use cases
3. IVF: Inverted File, good balance of speed and memory

Use Cases
1. Semantic Search: Find documents similar to a query
2. Recommendation Systems: Suggest products based on user preferences
3. RAG Systems: Retrieve relevant documents for LLM context
4. Duplicate Detection: Find similar items in large datasets
5. Image Search: Find visually similar images

Best Practices
1. Choose appropriate embedding model for your domain
2. Normalize vectors if using cosine similarity
3. Implement proper indexing for large datasets
4. Monitor query latency and optimize index parameters
5. Keep embeddings updated as new data arrives

Conclusion
Vector databases are essential infrastructure for AI applications. Choosing the right vector database depends on your scale, latency requirements, and filtering needs.`,
  } as DocumentFixture,

  /**
   * Short paragraph for quick testing
   * ~100 tokens
   */
  short_text: {
    filename: "short-article.txt",
    mimeType: "text/plain",
    content: `Artificial Intelligence and Machine Learning

Artificial Intelligence (AI) refers to computer systems designed to perform tasks that typically require human intelligence. Machine Learning (ML) is a subset of AI where systems learn from data without being explicitly programmed.

Key differences:
- AI: Broad field covering robots, expert systems, and automation
- ML: Focuses on statistical learning from data

Both are transforming industries from healthcare to finance.`,
  } as DocumentFixture,

  /**
   * Medium technical content
   * ~400 tokens
   */
  embedding_models: {
    filename: "embedding-models-comparison.txt",
    mimeType: "text/plain",
    content: `Embedding Models Comparison: OpenAI vs Cohere vs Google

Choosing the right embedding model is crucial for RAG applications. Here's a detailed comparison:

OpenAI Embeddings
- text-embedding-3-large: 3072 dimensions, state-of-the-art quality
- text-embedding-3-small: 1536 dimensions, good balance of quality and speed
- text-embedding-ada-002: Legacy model, 1536 dimensions

Strengths:
- Excellent semantic understanding
- Well-tested in production
- Good for general-purpose applications
- Strong support for 25+ languages

Weaknesses:
- Higher cost compared to alternatives
- Fixed dimensions (no flexibility on 3-large)

Cohere Embeddings
- embed-v4.0: Latest model with configurable dimensions (256-1536)
- embed-english-v3.0: Optimized for English, 1024 dimensions
- embed-multilingual-v3.0: 100+ language support, 1024 dimensions

Strengths:
- Configurable dimensions for cost optimization
- Good multilingual support
- Competitive pricing
- Fast inference

Weaknesses:
- Newer model with less production history
- Limited maximum dimensions

Google Embeddings (Gecko/Gemini)
- gemini-embedding-001: Latest, 3072 dimensions
- text-embedding-004: Legacy, 768 dimensions

Strengths:
- Integration with Google Cloud ecosystem
- Good performance for code-related tasks
- Matryoshka Representation Learning support

Weaknesses:
- Newer platform with emerging ecosystem
- Limited language support compared to alternatives

Recommendation
For general RAG applications: Use OpenAI text-embedding-3-small
For multilingual support: Use Cohere embed-multilingual-v3.0
For cost optimization: Use Cohere embed-v4.0 with reduced dimensions
For Google ecosystem: Use Gemini embeddings

Performance Metrics
All modern embedding models achieve high performance on benchmark datasets. The choice should be based on:
1. Language requirements
2. Dimension preferences
3. Cost constraints
4. Latency requirements
5. Integration ecosystem`,
  } as DocumentFixture,
};

/**
 * Create a test document with custom content
 */
export function createTestDocument(
  filename: string,
  content: string,
  mimeType = "text/plain",
): DocumentFixture {
  return {
    filename,
    content,
    mimeType,
    size: Buffer.byteLength(content, "utf-8"),
  };
}

/**
 * Create a large document for testing chunking
 */
export function createLargeDocument(lines: number = 1000): DocumentFixture {
  const content = Array(lines)
    .fill(null)
    .map(
      (_, i) =>
        `Line ${i + 1}: This is a test line with some content for testing document chunking and ingestion. `,
    )
    .join("");

  return {
    filename: `large-document-${nanoid(6)}.txt`,
    content,
    mimeType: "text/plain",
    size: Buffer.byteLength(content, "utf-8"),
  };
}

/**
 * Estimate tokens in text (rough approximation: 1 token â‰ˆ 4 characters)
 */
export function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}
